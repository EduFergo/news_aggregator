# News Multi-Scraper (news_aggregator)

Este proyecto es un agregador de noticias desarrollado con Scrapy que extrae, limpia y unifica datos de tres fuentes espa√±olas: **ABC**, **El Pa√≠s** y **La Voz de Galicia**.

## üõ†Ô∏è Detalles T√©cnicos
El proyecto utiliza diferentes estrategias de extracci√≥n seg√∫n los requisitos del ejercicio:

- **ABC (`abc.py`)**: Implementado exclusivamente con selectores **CSS**.
- **El Pa√≠s (`elpais.py`)**: Implementado exclusivamente con selectores **XPath**.
- **La Voz de Galicia (`lvdg.py`)**: Utiliza una combinaci√≥n de selectores y una l√≥gica especial de filtrado para descartar art√≠culos sin contenido (spam).

## üìä Estructura de Datos
Cada noticia extra√≠da se normaliza mediante un `Pipeline` y contiene los siguientes campos:
- `position`: Orden de aparici√≥n en la portada.
- `title`: T√≠tulo de la noticia (limpio de espacios y caracteres especiales).
- `date`: Fecha normalizada al formato `YYYY-MM-DD HH:MM`.
- `author`: Nombre del autor o autores.
- `source`: Nombre del peri√≥dico de origen.
- `url`: Enlace directo a la noticia (usado para depuraci√≥n y control de calidad).

## ‚ú® Procesamiento y Limpieza

1. **Normaliza Fechas:** Traduce meses del espa√±ol al ingl√©s y maneja formatos ISO y UTC autom√°ticamente usando `dateutil`.
2. **Limpieza de Texto:** Elimina saltos de l√≠nea, espacios extra y caracteres como `\xa0`.
3. **Validaci√≥n:** En `lvdg.py`, se descartan autom√°ticamente las entradas que no tienen t√≠tulo, autor ni fecha para asegurar la calidad del archivo final.

## üöÄ Ejecuci√≥n

1. **Instalar dependencias:**
   ```bash
   pip install scrapy python-dateutil

2. **Ejecutar todos los spiders:**
   ```bash
   python run_all.py

## üìÅ Archivos del Proyecto

### `spiders/`
Contiene los scripts de extracci√≥n espec√≠ficos para cada diario:

- **`abc.py`**: Extracci√≥n mediante **CSS**.
- **`elpais.py`**: Extracci√≥n mediante **XPath**.
- **`lvdg.py`**: Extracci√≥n mixta con l√≥gica de filtrado de noticias vac√≠as.

### `items.py`
Define el objeto **`NewsAggregatorItem`** con los campos:
- `position`
- `title`
- `date`
- `author`
- `source`
- `url`

### `pipelines.py`
Gestiona:
- La normalizaci√≥n de fechas (traducci√≥n de meses y formato ISO).
- La limpieza final de los datos.

### `run_all.py`
Script de conveniencia que utiliza **`CrawlerProcess`** para lanzar los tres spiders simult√°neamente y generar el archivo unificado.

### `all_news.json`
Resultado final que contiene todos los datos agregados en formato **JSON**.


